{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Natural Language Processing \n",
    "\n",
    "#### <I>Translation: Machine Learning with Unique Features and Feature Representations</I>\n",
    "\n",
    "### Guiding Principles\n",
    "#### Somehow we need to turn the meaning of the text into numerical values representative of their semantic importance\n",
    "#### Language is very noisy and contains a great dela of ambiguity therefore we need to find ways to reduce the noise and ambiguity\n",
    "\n",
    "\n",
    "## n-Gram Model\n",
    "\n",
    "   <img src=\"./images/n-gram.png\" width=\"400px\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Text to n-gram Frequency Counts\n",
    "\n",
    "##### 1) What does playing with ngram_range do?\n",
    "\n",
    "##### 2) What does max_df and min_df do?\n",
    "\n",
    "##### 3) What does max_featues do?\n",
    "\n",
    "##### 4) What does tokenizer do?\n",
    "\n",
    "##### 5) What does stop_words do?\n",
    "\n",
    "##### 6) What is the issue with larger n-grams?\n",
    "\n",
    "##### 7) Why are frequency counts considered a biased representation ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  21 \n",
      "\n",
      "Vocabulary:  ['an', 'bird', 'cat', 'catch', 'cow', 'do', 'dog', 'goat', 'how', 'know', 'not', 'old', 'she', 'spider', 'swallowed', 'the', 'there', 'to', 'was', 'who', 'woman'] \n",
      "\n",
      "Vectorized Count Matrix\n",
      "[[1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1]\n",
      " [0 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 2 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 2 0 1 0 0 0]\n",
      " [0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 2 0 1 0 0 0]\n",
      " [0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 2 0 1 0 0 0]\n",
      " [0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 2 0 1 0 0 0]] \n",
      "\n",
      "Totals:  [ 1  2  2  5  3  1  2  2  1  1  1  1  6  1  7 10  1  5  1  1  1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"There was an old woman who swallowed a cow\",\n",
    "        \"I do not know how she swallowed a cow\",\n",
    "        \"She swallowed the cow to catch the goat\",\n",
    "        \"She swallowed the goat to catch the dog\",\n",
    "        \"She swallowed the dog to catch the cat\",\n",
    "        \"She swallowed the cat to catch the bird\",\n",
    "        \"She swallowed the bird to catch the spider\"]\n",
    "                \n",
    "# max_df - ignore terms with a document frequency greater that threshold\n",
    "# min_df - ignore terms with a document frequency lower than threshold\n",
    "\n",
    "# What does token_pattern do by default?\n",
    "\n",
    "cv = CountVectorizer(lowercase=True, stop_words=None, max_features=None, ngram_range=(1, 1), max_df=1.0, min_df=1)\n",
    "\n",
    "cv_fit=cv.fit_transform(text)\n",
    "\n",
    "print(\"Vocabulary Size: \", len(cv.get_feature_names()), \"\\n\")\n",
    "print(\"Vocabulary: \", cv.get_feature_names(), \"\\n\")\n",
    "print(\"Vectorized Count Matrix\")\n",
    "print(cv_fit.toarray(), \"\\n\")\n",
    "print(\"Totals: \", cv_fit.toarray().sum(axis=0), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Text to a Co-occurance Matrix\n",
    "\n",
    "   <img src=\"./images/co-occurance-matrix.png\" width=\"300px\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['an', 'bird', 'cat', 'catch', 'cow', 'do', 'dog', 'goat', 'how', 'know', 'not', 'old', 'she', 'spider', 'swallowed', 'the', 'there', 'to', 'was', 'who', 'woman'] \n",
      "\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  1  0  0  1  0  1  0  1  1  1]\n",
      " [ 0  0  1  2  0  0  0  0  0  0  0  0  2  1  2  4  0  2  0  0  0]\n",
      " [ 0  1  0  2  0  0  1  0  0  0  0  0  2  0  2  4  0  2  0  0  0]\n",
      " [ 0  2  2  0  1  0  2  2  0  0  0  0  5  1  5 10  0  5  0  0  0]\n",
      " [ 1  0  0  1  0  1  0  1  1  1  1  1  2  0  3  2  1  1  1  1  1]\n",
      " [ 0  0  0  0  1  0  0  0  1  1  1  0  1  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  1  2  0  0  0  1  0  0  0  0  2  0  2  4  0  2  0  0  0]\n",
      " [ 0  0  0  2  1  0  1  0  0  0  0  0  2  0  2  4  0  2  0  0  0]\n",
      " [ 0  0  0  0  1  1  0  0  0  1  1  0  1  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  1  0  0  1  0  1  0  1  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  1  0  0  1  1  0  0  1  0  1  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  1  0  1  1  1]\n",
      " [ 0  2  2  5  2  1  2  2  1  1  1  0  0  1  6 10  0  5  0  0  0]\n",
      " [ 0  1  0  1  0  0  0  0  0  0  0  0  1  0  1  2  0  1  0  0  0]\n",
      " [ 1  2  2  5  3  1  2  2  1  1  1  1  6  1  0 10  1  5  1  1  1]\n",
      " [ 0  4  4 10  2  0  4  4  0  0  0  0 10  2 10  0  0 10  0  0  0]\n",
      " [ 1  0  0  0  1  0  0  0  0  0  0  1  0  0  1  0  0  0  1  1  1]\n",
      " [ 0  2  2  5  1  0  2  2  0  0  0  0  5  1  5 10  0  0  0  0  0]\n",
      " [ 1  0  0  0  1  0  0  0  0  0  0  1  0  0  1  0  1  0  0  1  1]\n",
      " [ 1  0  0  0  1  0  0  0  0  0  0  1  0  0  1  0  1  0  1  0  1]\n",
      " [ 1  0  0  0  1  0  0  0  0  0  0  1  0  0  1  0  1  0  1  1  0]]\n"
     ]
    }
   ],
   "source": [
    "Xc = (cv_fit.T * cv_fit) # this is co-occurrence matrix in sparse csr format\n",
    "Xc.setdiag(0) # typically you want to fill same word cooccurence to 0\n",
    "print(\"Vocabulary: \", cv.get_feature_names(), \"\\n\")\n",
    "print(Xc.todense()) # print out matrix in dense format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term-Frequency-Inverse Document Frequency\n",
    "\n",
    "#### Inverse Document Frequency  $(idf_t)$\n",
    "* <I>Measure of <B>informativeness</B> of a term: it's rarity across the whole corpus.</I>\n",
    "\n",
    "     ###   $idf_t\\space=\\space log_{10}(N\\space/\\space df)$<BR>\n",
    "     \n",
    "* <I>Assign a <B>tf.idf</B> weight to each term <B>t</B> in each document <B>d</B>\n",
    "\n",
    "    ### $w_{t,d}\\space=\\space tf_{t,d}\\space x \\space log_{10}(N\\space/\\space df)$<BR>\n",
    "\n",
    "#### <I><U><B>Intuitively</B></U></I>\n",
    "* <I>Weight increases with the number of occurrences within a document</I>\n",
    "* <I>Weight increases with the rarity of the term across the whole corpus</I>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** STORED AS A SPARSE MATRIX ****\n",
      "\n",
      "Tf-idf Matrix Size:  (7, 47) \n",
      "\n",
      "Tf-Idf Matrix\n",
      "[[0.27350507 0.27350507 0.         0.         0.         0.\n",
      "  0.         0.         0.19406002 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.27350507 0.27350507\n",
      "  0.         0.         0.         0.11461498 0.2270327  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.27350507 0.27350507 0.         0.         0.27350507\n",
      "  0.27350507 0.27350507 0.27350507 0.27350507 0.27350507]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.22642736 0.         0.31912307 0.31912307\n",
      "  0.         0.         0.         0.         0.31912307 0.31912307\n",
      "  0.31912307 0.31912307 0.31912307 0.31912307 0.         0.\n",
      "  0.15158902 0.15158902 0.         0.13373164 0.26489955 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.20210075 0.20210075 0.2657382  0.37452714 0.         0.\n",
      "  0.         0.         0.31088969 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.17790692 0.17790692 0.         0.15694926 0.         0.20210075\n",
      "  0.40420151 0.         0.         0.37452714 0.         0.31088969\n",
      "  0.         0.         0.         0.20210075 0.20210075 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.20390126 0.20390126 0.         0.         0.         0.\n",
      "  0.31365939 0.         0.31365939 0.37786378 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.17949188 0.17949188 0.         0.15834752 0.         0.20390126\n",
      "  0.40780251 0.         0.         0.         0.31365939 0.31365939\n",
      "  0.         0.         0.         0.20390126 0.20390126 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.31365939 0.\n",
      "  0.20390126 0.20390126 0.         0.         0.         0.\n",
      "  0.31365939 0.37786378 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.17949188 0.17949188 0.         0.15834752 0.         0.20390126\n",
      "  0.40780251 0.         0.31365939 0.         0.31365939 0.\n",
      "  0.         0.         0.         0.20390126 0.20390126 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.31365939 0.         0.31365939 0.37786378\n",
      "  0.20390126 0.20390126 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.17949188 0.17949188 0.         0.15834752 0.         0.20390126\n",
      "  0.40780251 0.31365939 0.31365939 0.         0.         0.\n",
      "  0.         0.         0.         0.20390126 0.20390126 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.30059704 0.36212765 0.         0.\n",
      "  0.19540979 0.19540979 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.17201694 0.17201694 0.36212765 0.15175313 0.         0.19540979\n",
      "  0.39081958 0.30059704 0.         0.         0.         0.\n",
      "  0.36212765 0.         0.         0.19540979 0.19540979 0.\n",
      "  0.         0.         0.         0.         0.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Note that max-df is set such that we keep words that can appear across 100% of the corpus and min-df is set\n",
    "# such that we throw away words that are in less than 10% of our document corpus. \n",
    "# We can also generate unigrams and bigrams which is about right for clinical notes. Beyond bigrams we get very sparse....\n",
    "\n",
    "tfidfVectorizer = TfidfVectorizer(max_df=1.0, max_features=200000,\n",
    "                                  min_df=0.1, stop_words=None,\n",
    "                                  use_idf=True, tokenizer=None, ngram_range=(1,2))   \n",
    "\n",
    "tfidfMatrix = tfidfVectorizer.fit_transform(text)\n",
    "\n",
    "print(\"**** STORED AS A SPARSE MATRIX ****\\n\")\n",
    "print(\"Tf-idf Matrix Size: \", tfidfMatrix.shape, \"\\n\")\n",
    "print(\"Tf-Idf Matrix\")\n",
    "print(tfidfMatrix.toarray())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Feature Representations . . .\n",
    "\n",
    "#### <U>Sentence Level Annotation</U>\n",
    "##### Example: \n",
    "* (+) There is a good deal of opacification at the left base.\n",
    "* (-) Right lung is clear\n",
    "* (?) CHEST ONE VIEW PORTABLE\n",
    "<BR>\n",
    "   \n",
    "* <B>Advantages:</B> Finer grained accuracy\n",
    "* <B>Disadvatages:</B> Requires annotation and a voting method\n",
    "\n",
    "#### <U>Concept-Assertion Pairs</U>\n",
    "##### Example: reported fever  present, wheezing absent, . . .\n",
    "* <B>Advantages:</B> Very accurate and accounts for negation\n",
    "* <B>Disadvatages:</B> Requires very fine-grained annotation and a voting method\n",
    "\n",
    "#### <U>Noun, Verb, and Prepositional Phrases</U>\n",
    "##### Example: no pleural effusion, bilateral infiltrates, lungs are clear . . . \n",
    "* <B>Advantages:</B> Accurate and can account for negation\n",
    "* <B>Disadvatages:</B> Requires a good dependency parser, determination of phrase length, and a voting method\n",
    "\n",
    "#### <U>IOB (Inside-Outside-Beginning) Format</U>\n",
    "##### Example:\n",
    "* I_O complained_O to_O Microsoft_B-ORG about_O Bill_B-PER Gates_I-PER\n",
    "* They_O told_O me_O to_O see_O the_O mayor_O of_O New_B-LOC York_I-LOC\n",
    "<BR>\n",
    "\n",
    "* <B>Advantages:</B> Very fine-grained annotation\n",
    "* <B>Disadvatages:</B> Requires annotation !!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
